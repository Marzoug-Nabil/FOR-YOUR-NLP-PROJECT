{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e30f2908-5f8d-4984-af8b-42ae1a844b05",
   "metadata": {},
   "source": [
    "## Notes for your NLP Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b02feb3-6bbb-41d0-ad64-6fb8b171ae29",
   "metadata": {},
   "source": [
    "This notebook will clarify some of the most important concepts you need to know when your dealing with nlp project. Happy learning !!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74552bb-f312-430b-a1f2-f66f3fa718d1",
   "metadata": {},
   "source": [
    "### Word Embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a6eda2-ff56-4ec2-b78f-62b70cc35a13",
   "metadata": {},
   "source": [
    "1. Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa771d5f-fe21-4304-8d8e-3631238ed8e0",
   "metadata": {},
   "source": [
    "Given a supervised learning task to predict which tweets are about real disasters and which ones are not (classification). Here the independent variable would be the tweets (text) and the target variable would be the binary values (1: Real Disaster, 0: Not real Disaster).\n",
    "\n",
    "Now, Machine Learning and Deep Learning algorithms only take numeric input. So, how do we convert tweets to their numeric values? We will dive deep into the techniques to solve such problems, but first let’s look at the solution provided by word embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9a551c-a4e4-4ddf-8721-70ce31c4b971",
   "metadata": {},
   "source": [
    "2. Solution\n",
    "\n",
    "***Word Embeddings*** in NLP is a technique where individual words are represented as real-valued vectors in a lower-dimensional space and captures inter-word semantics. Each word is represented by a real-valued vector with tens or hundreds of dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59030667-7891-4d1a-9005-9386053e06be",
   "metadata": {},
   "source": [
    "### 1. Term frequency-inverse document frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9915cce8-917d-49b4-ba13-02f1114bf58e",
   "metadata": {},
   "source": [
    "***Term Frequency-Inverse Document Frequency (TF-IDF)*** is a numerical statistic used in natural language processing and information retrieval to evaluate the importance of a term within a document relative to its importance across a collection of documents. TF-IDF is a key concept in document analysis and text mining, helping to identify the significance of words or phrases within a body of text.\n",
    "\n",
    "Here's a breakdown of the components of TF-IDF:\n",
    "\n",
    "***Term Frequency (TF):*** This measures how frequently a term (word or phrase) occurs within a specific document. It is calculated by counting the number of times the term appears and is often normalized to account for the document's length, preventing longer documents from having higher TF values.\n",
    "\n",
    "This term is calculated like this : $$\\text{term frequencey (i,j)} =  \\frac{{\\text{Term i frequency in document } j}}{{\\text{Total number of terms in document } j}}\n",
    "  $$\n",
    "\n",
    "***Inverse Document Frequency (IDF):*** This evaluates the rarity of a term across a collection of documents. The IDF score is determined by dividing the total number of documents by the number of documents containing the term, followed by applying a logarithmic function. The idea is that terms that appear in fewer documents are more valuable.\n",
    "\n",
    "This term is calculated like this: $$ \\text{Inverse document frequency (i)} = \\log({\\frac{{\\text{Total documents } j}}{{\\text{Number of documanets containing term } i}}}) $$\n",
    "\n",
    "The TF-IDF is calculated using the following formula: $$ \\text{term frequencey (i,j)}*\\text{Inverse document frequency (i)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada5acfe-8455-4e88-8320-2b009b4c8795",
   "metadata": {},
   "source": [
    "## Disadvantages of TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0076449a-2527-46af-b959-eb645aba2b5c",
   "metadata": {},
   "source": [
    "While Term Frequency-Inverse Document Frequency (TF-IDF) is a widely used method in natural language processing for information retrieval and text mining, it also has certain limitations. Some disadvantages of TF-IDF include:\n",
    "\n",
    "* ***Inability to Capture Word Semantics:*** TF-IDF doesn't consider the semantics or meaning of words. It treats each word as an independent entity, disregarding their context or relationships within the text. This might lead to limited accuracy when dealing with the semantics of different words or phrases.\n",
    "\n",
    "* ***High Dimensionality:*** In datasets with a vast vocabulary or a large number of documents, TF-IDF can generate high-dimensional matrices, leading to computational complexity and memory storage issues.\n",
    "\n",
    "* ***Sensitivity to Stop Words and Rare Terms:*** Stop words (frequently occurring common words like 'and', 'the', 'of') might be assigned significant weights due to high term frequency but low inverse document frequency. Additionally, rare terms with limited occurrences might not have enough information for TF-IDF to accurately assess their relevance.\n",
    "\n",
    "* ***Sparsity and Zero Values:*** In sparse datasets, where many terms might not occur frequently in documents, TF-IDF can produce sparse matrices with numerous zero values. This sparsity can impact the performance of some machine learning models and techniques that require dense matrices.\n",
    "\n",
    "* ***No Word Order or Sequence Information:***  TF-IDF treats documents as a bag of words, disregarding word order and sequence information, which could be crucial in certain NLP tasks like sentiment analysis or language generation.\n",
    "\n",
    "* ***Dependency on Document Length:*** TF-IDF tends to favor longer documents as they might contain more instances of terms. This could introduce a bias toward longer documents in the analysis.\n",
    "\n",
    "* ***Unsuitable for Some NLP Tasks:*** In tasks like word embedding or language modeling where a richer representation of words is required, TF-IDF might not be the ideal approach due to its inability to capture complex word relationships and representations.\n",
    "\n",
    "* ***Not Suitable for Multi-Word Expressions:*** TF-IDF operates on individual words, not considering multi-word expressions or phrases as single entities, limiting its effectiveness in capturing the meaning of multi-word units.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27203fed-820e-419e-a84d-2af78bb8f1b3",
   "metadata": {},
   "source": [
    "### TF-IDF in Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97d57076-0e5c-483b-a422-ef13d9dcd182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "# Create a TfidfVectorizer instance\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the corpus using TF-IDF\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get the feature names (terms) in the order they appear in the matrix\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the TF-IDF matrix to a dense array\n",
    "tfidf_matrix_dense = tfidf_matrix.toarray()\n",
    "\n",
    "# Create a DataFrame for better visualization (optional)\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data=tfidf_matrix_dense, columns=feature_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c086c113-53db-4f7c-9816-1bdeeeee6431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.469791</td>\n",
       "      <td>0.580286</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.687624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538648</td>\n",
       "      <td>0.281089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.511849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267104</td>\n",
       "      <td>0.511849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267104</td>\n",
       "      <td>0.511849</td>\n",
       "      <td>0.267104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.469791</td>\n",
       "      <td>0.580286</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        and  document     first        is       one    second       the  \\\n",
       "0  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
       "1  0.000000  0.687624  0.000000  0.281089  0.000000  0.538648  0.281089   \n",
       "2  0.511849  0.000000  0.000000  0.267104  0.511849  0.000000  0.267104   \n",
       "3  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
       "\n",
       "      third      this  \n",
       "0  0.000000  0.384085  \n",
       "1  0.000000  0.281089  \n",
       "2  0.511849  0.267104  \n",
       "3  0.000000  0.384085  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b946d4-0a04-4477-9c0c-eb6946250029",
   "metadata": {},
   "source": [
    "The rows represent each document, the columns represent the vocabulary, and the values of tf-idf(i,j) are obtained through the above formula. This matrix obtained can be used along with the target variable to train a machine learning/deep learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6c2014-4e6b-4e54-986a-2c58675d8692",
   "metadata": {},
   "source": [
    "To see how we can use this method for machine learning or deep learning task i recommend visiting the following [website](https://www.analyticsvidhya.com/blog/2021/09/creating-a-movie-reviews-classifier-using-tf-idf-in-python/) and read the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21acbe3-dfbe-422a-a8d3-3d92e82b9c99",
   "metadata": {},
   "source": [
    "### 2. Bag of words (BOW)\n",
    "***The \"Bag of Words\"*** (BoW) is a simple and fundamental technique used in natural language processing (NLP) and text analysis. It's a way to represent text data as a collection of words or tokens, ignoring the order and structure of the words in a document. The name \"Bag of Words\" suggests that you're treating the text as a \"bag\" of individual words, with no specific order, grammar, or context, just like counting how many times each word appears in a bag.\n",
    "\n",
    "Here's how the Bag of Words technique works:\n",
    "\n",
    "1. ***Tokenization:***  The first step is to break down a text document into individual words or tokens. This process involves removing punctuation and splitting the text into words.\n",
    "2. ***Vocabulary Building:***  Create a vocabulary that includes all the unique words (tokens) in the entire corpus of documents. Each word in the vocabulary is assigned a unique index.\n",
    "3. ***Counting Word Occurrences:*** For each document in the corpus, count how many times each word from the vocabulary appears in that document. This creates a numerical representation of the document in the form of a vector, where each dimension corresponds to a word in the vocabulary, and the value in each dimension is the count of that word's occurrence in the document.\n",
    "4. ***Sparse Representation:*** Since most documents contain only a subset of the words from the vocabulary, the resulting vectors are usually sparse, with many dimensions having a value of zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c79ca4-c6c7-42ac-99d6-40784ff299d1",
   "metadata": {},
   "source": [
    "### BOW In Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "656d9990-0ec6-4d5a-b010-af678ed6e9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nabil/.local/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample documents\n",
    "corpus = [\n",
    "    \"The quick brown fox\",\n",
    "    \"Jumped over the lazy dog\",\n",
    "    \"The brown dog barked\"\n",
    "]\n",
    "\n",
    "# Create a CountVectorizer instance\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the corpus using BoW\n",
    "bow_matrix = count_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get the feature names (words) in the order they appear in the matrix\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "\n",
    "# Convert the BoW matrix to a dense array\n",
    "bow_matrix_dense = bow_matrix.toarray()\n",
    "\n",
    "# Create a DataFrame for better visualization (optional)\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data=bow_matrix_dense, columns=feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94bdfbbf-64e3-40eb-9273-da9a9d01dae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>barked</th>\n",
       "      <th>brown</th>\n",
       "      <th>dog</th>\n",
       "      <th>fox</th>\n",
       "      <th>jumped</th>\n",
       "      <th>lazy</th>\n",
       "      <th>over</th>\n",
       "      <th>quick</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   barked  brown  dog  fox  jumped  lazy  over  quick  the\n",
       "0       0      1    0    1       0     0     0      1    1\n",
       "1       0      0    1    0       1     1     1      0    1\n",
       "2       1      1    1    0       0     0     0      0    1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a10f44a-3287-42eb-9b9f-522ca9ea4934",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430e4ba2-6f89-42c0-a2bf-72d5ab5d72eb",
   "metadata": {},
   "source": [
    "`Word2Vec` is a popular and widely used word embedding technique in natural language processing (NLP). It was introduced by Tomas Mikolov and his team at Google in 2013. Word2Vec is a neural network-based model that learns to represent words as dense vectors in a continuous vector space. These dense word vectors, also known as word embeddings, capture the semantic and syntactic relationships between words, making them useful for a wide range of NLP tasks.\n",
    "\n",
    "The key idea behind Word2Vec is the \"distributional hypothesis,\" which suggests that words that appear in similar contexts often have similar meanings. Word2Vec leverages this idea by learning word embeddings from large text corpora using neural networks. There are two primary architectures for Word2Vec:\n",
    "\n",
    "* ***Continuous Bag of Words (CBOW)***: In the CBOW model, the goal is to predict the target word given the context words. The context words are represented by their word vectors, and the target word is predicted based on these representations.\n",
    "\n",
    "* ***Skip-gram***: In the skip-gram model, the goal is to predict the context words (words that occur in the neighborhood of the target word) given the target word. The target word is represented by its word vector, and the context words are predicted based on this representation.\n",
    "\n",
    "Word2Vec models are typically trained on large text corpora, such as Wikipedia, news articles, or web pages. Training Word2Vec models involves adjusting the word vectors so that they accurately predict the words that appear in their contexts.\n",
    "\n",
    "Once trained, Word2Vec word embeddings can be used for various NLP tasks, including:\n",
    "\n",
    "* ****Word Similarity****: You can compute the similarity between words by measuring the cosine similarity between their word vectors.\n",
    "\n",
    "* ****Analogical Reasoning****: Word2Vec enables you to perform word analogy tasks, such as \"king - man + woman = queen,\" by using vector arithmetic.\n",
    "\n",
    "* ****Text Classification****: Word embeddings can be used as features for text classification tasks, like sentiment analysis or topic classification.\n",
    "\n",
    "Information Retrieval: Word embeddings can help improve search engine results by capturing semantic relationships between words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e80e020-d558-4874-b4f9-445ff2e766a9",
   "metadata": {},
   "source": [
    "You can use popular libraries like Gensim or TensorFlow to train Word2Vec models. Here's how to do it using Gensim.\n",
    "In this tutorial we are going to word with the gensim library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7778cdbf-398f-4c8a-8e69-1105ad7bd013",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m308.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.5 in /usr/lib/python3/dist-packages (from gensim) (1.21.5)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m331.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.7.0 in /usr/lib/python3/dist-packages (from gensim) (1.8.0)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.3.2 smart-open-6.4.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fc8e55f-2e4d-4379-95b2-2fcfe1351c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Define your corpus\n",
    "corpus = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    " ['this', 'is', 'the', 'second', 'sentence'],\n",
    " ['yet', 'another', 'sentence'],\n",
    " ['one', 'more', 'sentence'],\n",
    " ['and', 'the', 'final', 'sentence']]\n",
    "\n",
    "# Initialize and train the Word2Vec model\n",
    "model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "# Parameters:\n",
    "# - `vector_size`: The dimensionality of the word vectors.\n",
    "# - `window`: The maximum distance between the current and predicted word within a sentence.\n",
    "# - `min_count`: Ignores all words with a total frequency lower than this.\n",
    "# - `sg` (Skip-gram): Use the Skip-gram architecture (set to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5acf5d89-1965-4e8e-8e6e-c0511f8f5752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=14, vector_size=100, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "# summarize the loaded model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5498d412-59db-4578-b758-18db0b216f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentence',\n",
       " 'the',\n",
       " 'is',\n",
       " 'this',\n",
       " 'final',\n",
       " 'and',\n",
       " 'more',\n",
       " 'one',\n",
       " 'another',\n",
       " 'yet',\n",
       " 'second',\n",
       " 'word2vec',\n",
       " 'for',\n",
       " 'first']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summarize vocabulary\n",
    "words = list(model.wv.index_to_key)\n",
    "words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b32deb75-51be-40ac-a8e7-ee105278b161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.3622725e-04,  2.3643016e-04,  5.1033497e-03,  9.0092728e-03,\n",
       "       -9.3029495e-03, -7.1168090e-03,  6.4588715e-03,  8.9729885e-03,\n",
       "       -5.0154282e-03, -3.7633730e-03,  7.3805046e-03, -1.5334726e-03,\n",
       "       -4.5366143e-03,  6.5540504e-03, -4.8601604e-03, -1.8160177e-03,\n",
       "        2.8765798e-03,  9.9187379e-04, -8.2852151e-03, -9.4488189e-03,\n",
       "        7.3117660e-03,  5.0702621e-03,  6.7576934e-03,  7.6286553e-04,\n",
       "        6.3508893e-03, -3.4053659e-03, -9.4640255e-04,  5.7685734e-03,\n",
       "       -7.5216386e-03, -3.9361049e-03, -7.5115822e-03, -9.3004224e-04,\n",
       "        9.5381187e-03, -7.3191668e-03, -2.3337698e-03, -1.9377422e-03,\n",
       "        8.0774352e-03, -5.9308959e-03,  4.5161247e-05, -4.7537349e-03,\n",
       "       -9.6035507e-03,  5.0072931e-03, -8.7595871e-03, -4.3918253e-03,\n",
       "       -3.5099984e-05, -2.9618264e-04, -7.6612402e-03,  9.6147414e-03,\n",
       "        4.9820566e-03,  9.2331432e-03, -8.1579182e-03,  4.4957972e-03,\n",
       "       -4.1370774e-03,  8.2453492e-04,  8.4986184e-03, -4.4621779e-03,\n",
       "        4.5175003e-03, -6.7869616e-03, -3.5484887e-03,  9.3985079e-03,\n",
       "       -1.5776539e-03,  3.2137157e-04, -4.1406299e-03, -7.6826881e-03,\n",
       "       -1.5080094e-03,  2.4697948e-03, -8.8802812e-04,  5.5336617e-03,\n",
       "       -2.7429771e-03,  2.2600652e-03,  5.4557943e-03,  8.3459523e-03,\n",
       "       -1.4537406e-03, -9.2081428e-03,  4.3705511e-03,  5.7178497e-04,\n",
       "        7.4419067e-03, -8.1328390e-04, -2.6384138e-03, -8.7530091e-03,\n",
       "       -8.5655687e-04,  2.8265619e-03,  5.4014279e-03,  7.0526553e-03,\n",
       "       -5.7031228e-03,  1.8588186e-03,  6.0888622e-03, -4.7980524e-03,\n",
       "       -3.1072616e-03,  6.7976285e-03,  1.6314745e-03,  1.8991709e-04,\n",
       "        3.4736372e-03,  2.1777629e-04,  9.6188262e-03,  5.0606038e-03,\n",
       "       -8.9173913e-03, -7.0415614e-03,  9.0145587e-04,  6.3925339e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# access vector for one word\n",
    "vector = model.wv['sentence']\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c631b5-9837-495e-90dd-872c3fff3fcb",
   "metadata": {},
   "source": [
    "### Visualize Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99310c82-35ef-4f90-a981-71a962d43ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [model.wv[word] for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2c95677-738a-4d2e-a571-ce897ee0d83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69b4e208-8832-40a2-b8e3-8cd5e2479121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtEAAAGbCAYAAAAC4syQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA1LElEQVR4nO3de3SV1Z3/8fc2BIiiBUdUCDpSiygESCAgiiKCGlCL0WqrpVVrvc2MtTrT1FCWjk6XLb/itJZ6YVCxtkXFCwV/iuIVUYpI0iCCigLGS2AUtSAoYBL27w8O+QUMykMuJwnv11pn5Tz72fuc7zl7AR+e7LNPiDEiSZIkadftle4CJEmSpJbGEC1JkiQlZIiWJEmSEjJES5IkSQkZoiVJkqSE2qS7gN1xwAEHxMMOOyzdZUiSJKmVKy0t/SjG2HnH9hYZog877DBKSkrSXYYkSZJauRDCO3W1u5xDkiRJSsgQLUmSJCVkiJYkSZISMkRLkiRJCRmiJUmSpIQM0ZIkSVJChmhJkiQpIUO0JEmSlJAhWpIkSTUmTpzIUUcdRadOnRg/fvwujysvL+fee+9txMqalxb5jYWSJElqHLfddhuPP/443bt3r/N8VVUVbdp8OUJuC9Hf//73G7vEZsEQLUmSJAAuv/xyVq5cyejRo7noootYsWIFt9xyCxdeeCH7778/ZWVl9O/fn9GjR/PTn/4UgBACc+fOpbi4mNdff53c3FwuuOACrr766jS/msZliJYkSRIAkyZN4oknnuC5557j0Ucf3e7cm2++ydNPP01GRgbf/va3ufXWWxkyZAgbNmygffv2jB8/nptuuulL41or10RLkiTpa51zzjlkZGQAMGTIEP793/+diRMnsnbt2jqXd7R2hmhJkqQ93IyyCoaMf5buxY/xv+s2MWvx6i/12WeffWruFxcXc+edd7Jx40YGDx7MG2+80ZTlNgt73n8bJEmSVGNGWQVjp7/KxspqAKq2RH752GuM2u8fOx2zYsUK+vTpQ58+fZg/fz5vvPEGhxxyCOvXr2+qstPOK9GSJEl7sAmzl9UE6G02VVbz+JIvX43e5uabbyYnJ4d+/fqRlZXFqFGj6Nu3L23atKFfv3787ne/a+yy0y7EGNNdQ2L5+fmxpKQk3WVIkiS1eN2LH6OuNBiAt8ef1tTlNDshhNIYY/6O7V6JliRJ2oN17ZiVqF1bGaIlSZL2YEUFPcnKzNiuLSszg6KCnmmqqGXwg4WSJEl7sMK8bGDr2uhVazfStWMWRQU9a9pVN0O0JEnSHq4wL9vQnJDLOSRJkqSEDNGSJElSQoZoSZIkKSFDtCRJkpSQIVqSJElKyBAtSZIkJWSIliRJkhJqkBAdQhgZQlgWQlgeQiiu43wIIUxMnV8cQuifam8fQng5hPBKCGFpCOGGhqhHkiRJakz1DtEhhAzgVmAU0As4L4TQa4duo4AeqdulwO2p9s3A8BhjPyAXGBlCGFzfmiRJkqTG1BBXogcBy2OMK2OMXwD3A2fs0OcM4E9xq5eAjiGELqnjDak+malbbICaJEmSpEbTECE6G3iv1vH7qbZd6hNCyAghLAI+BJ6KMS6o60lCCJeGEEpCCCVr1qxpgLIlSZKk3dMQITrU0bbj1eSd9okxVscYc4FuwKAQQk5dTxJjnBxjzI8x5nfu3Lk+9UqSJEn10hAh+n3gkFrH3YBVSfvEGNcCc4CRDVCTJEmS1GgaIkQvBHqEELqHENoC5wKP7NDnEeD81C4dg4F1McbVIYTOIYSOACGELOAk4I0GqEmSJElqNG3q+wAxxqoQwhXAbCADmBJjXBpCuDx1fhIwCzgVWA58DvwoNbwLcE9qh4+9gAdijI/WtyZJkiSpMYUYW95mGPn5+bGkpCTdZUiSJKmVCyGUxhjzd2z3GwslSZKkhAzRkiRJUkKGaEmSJCkhQ7QkSZKUkCFakiRJSsgQLUmSJCVkiJYkSZISMkRLkiRJCRmiJUmSpIQM0ZIkSVJChmhJkiQpIUO0JEmSlJAhWpIkSUrIEC1JkiQlZIiWJEmSEjJES5IkSQkZoiVJkqSEDNGSJElSQoZoSZIkKSFDtCRJkpSQIVqSJElKyBAtSZIkJWSIliRJkhIyREuSJEkJGaIlSZKkhAzRkiRJUkKGaEmSJCkhQ7QkACZOnMhRRx3FmDFj0l2KJEnNXpt0FyCpebjtttt4/PHH6d69+9f2raqqok0b//qQJO25/FdQEpdffjkrV65k9OjRXHjhhbzwwgusXLmSvffem8mTJ9O3b1+uv/56Vq1aRXl5OQcccAD33ntvusuWJCltXM4hiUmTJtG1a1eee+45ysvLycvLY/HixfzqV7/i/PPPr+lXWlrKzJkzDdCSpD2eV6KlPdiMsgomzF7GqrUb+d91m5i1eDUvvvgiDz/8MADDhw/n448/Zt26dQCMHj2arKysdJYsSVKzYIiW9lAzyioYO/1VNlZWA1C1JfLLx16j6vMvvtQ3hADAPvvs06Q1SpLUXLmcQ9pDTZi9rCZAb7OpsppN/9STqVOnAjBnzhwOOOAA9ttvv3SUKElSs+WVaGkPtWrtxjrbMwd+l5KS++jbty97770399xzTxNXJklS8xdijOmuIbH8/PxYUlKS7jKkFm3I+GepqCNIZ3fMYl7x8DRUJElS8xNCKI0x5u/Y7nIOaQ9VVNCTrMyM7dqyMjMoKuiZpookSWo5XM4h7aEK87IBanbn6Noxi6KCnjXtkiRp5wzR0h6sMC/b0CxJ0m5wOYckSZKUkCFakiRJSsgQLUmSJCVkiJYkSZISMkRLkiRJCRmiJUmSpIQM0ZIkSVJCDRKiQwgjQwjLQgjLQwjFdZwPIYSJqfOLQwj9U+2HhBCeCyG8HkJYGkL4aUPUI0mSJDWmeofoEEIGcCswCugFnBdC6LVDt1FAj9TtUuD2VHsV8B8xxqOAwcC/1TFWkiRJe7iJEydy1FFH0alTJ8aPH7/bj9OhQ4cGqachvrFwELA8xrgSIIRwP3AG8FqtPmcAf4oxRuClEELHEEKXGONqYDVAjHF9COF1IHuHsZIkSdrD3XbbbTz++ON079493aUADbOcIxt4r9bx+6m2RH1CCIcBecCCup4khHBpCKEkhFCyZs2a+tYsSZKkFuLyyy9n5cqVjB49mt/97ndcccUVAFx44YVceeWVHHvssXzzm9/koYceAmDDhg2MGDGC/v3706dPH2bOnNngNTVEiA51tMUkfUIIHYCHgatijJ/W9SQxxskxxvwYY37nzp13u1hJkiS1LJMmTaJr164899xzdOrUabtzq1ev5sUXX+TRRx+luHjrR/Pat2/PX//6V/7+97/z3HPP8R//8R9sXRDRcBpiOcf7wCG1jrsBq3a1Twghk60BemqMcXoD1CNJkqRWYEZZBRNmL2PV2o3877pNzFq8+kt9CgsL2WuvvejVqxcffPABADFGfvGLXzB37lz22msvKioq+OCDDzj44IMbrLaGCNELgR4hhO5ABXAu8P0d+jwCXJFaL300sC7GuDqEEIC7gNdjjL9tgFokSZLUCswoq2Ds9FfZWFkNQNWWyC8fe41R+/1ju37t2rWrub/tavPUqVNZs2YNpaWlZGZmcthhh7Fp06YGra/eyzlijFXAFcBs4HXggRjj0hDC5SGEy1PdZgErgeXAHcC/ptqHAD8EhocQFqVup9a3JkmSJLVsE2YvqwnQ22yqrObxJV++Gr2jdevWceCBB5KZmclzzz3HO++80+D1NcSVaGKMs9galGu3Tap1PwL/Vse4F6l7vbQkSZL2YKvWbqyz/R+fV37t2DFjxvDtb3+b/Px8cnNzOfLIIxu6PEJDL7JuCvn5+bGkpCTdZUiSJKmRDBn/LBV1BOnsjlnMKx7eZHWEEEpjjPk7tvu135IkSWp2igp6kpWZsV1bVmYGRQU901TR9hpkOYckSZLUkArztn6lyLbdObp2zKKooGdNe7oZoiVJktQsFeZlN5vQvCOXc0iSJEkJGaIlSZKkhAzRkiRJUkKGaEmSJCkhQ7QkSZKUkCFakiRJSsgQLUmSJCVkiJYkSZISMkRLkiRJCRmiJUmSpIQM0ZIkSVJChmhJkiQpIUO0JEmSlJAhWpIkSUrIEC1JkiQlZIiWJEmSEjJE6ystWrSIWbNmpbsMSZKkZsUQra9kiJYkSfoyQ3Qr9tlnn3HaaafRr18/cnJymDZtGqWlpZxwwgkMGDCAgoICVq9eDcCwYcO45pprGDRoEEcccQQvvPACX3zxBddddx3Tpk0jNzeXadOm8dlnn3HRRRcxcOBA8vLymDlzJgB//OMfOeussxg5ciQ9evTg5z//eU0dTzzxBP3796dfv36MGDGipra6HkeSJKklaJPuAtR4nnjiCbp27cpjjz0GwLp16xg1ahQzZ86kc+fOTJs2jXHjxjFlyhQAqqqqePnll5k1axY33HADTz/9NP/1X/9FSUkJt9xyCwC/+MUvGD58OFOmTGHt2rUMGjSIk046Cdh61bqsrIx27drRs2dPfvKTn9C+fXsuueQS5s6dS/fu3fnkk08AuPHGG+t8nH322ScN75QkSVIyhuhWZkZZBRNmL2PV2o10qtxAxazZ7H/NNZx++ul06tSJJUuWcPLJJwNQXV1Nly5dasaeddZZAAwYMIDy8vI6H//JJ5/kkUce4aabbgJg06ZNvPvuuwCMGDGCb3zjGwD06tWLd955h3/84x8MHTqU7t27A7D//vt/5eMcddRRDfyOSJIkNTxDdCsyo6yCsdNfZWNlNQCfZB7AN877bzbvu5qxY8dy8skn07t3b+bPn1/n+Hbt2gGQkZFBVVVVnX1ijDz88MP07Nlzu/YFCxbUjK/9GDFGQgi7/DiSJEktgWuiW5EJs5fVBGiAqvUfs5k2LGyTw89+9jMWLFjAmjVrakJ0ZWUlS5cu/crH3HfffVm/fn3NcUFBAX/4wx+IMQJQVlb2leOPOeYYnn/+ed5++22AmuUcSR9HkiSpOfFKdCuyau3G7Y4r15Tz4Zy7WR0CNx76T9x+++20adOGK6+8knXr1lFVVcVVV11F7969d/qYJ554IuPHjyc3N5exY8dy7bXXctVVV9G3b19ijBx22GE8+uijOx3fuXNnJk+ezFlnncWWLVs48MADeeqppxI/jiRJUnMStl0JbEny8/NjSUlJustodoaMf5aKHYI0QHbHLOYVD09DRZIkSS1bCKE0xpi/Y7vLOVqRooKeZGVmbNeWlZlBUYHrjiVJkhqSyzlakcK8bICa3Tm6dsyiqKBnTbskSZIahiG6lSnMyzY0S5IkNTKXc0iSJEkJGaIlSZKkhAzRkiRJUkKGaEmSJCkhQ7QkSZKUkCFakiRJSsgQLUmSJCVkiJYkSZISMkRLkiRJCRmiJUmSpIQM0ZIkSVJChmhJkiQpIUO0JEmSlJAhWpIkSUqoQUJ0CGFkCGFZCGF5CKG4jvMhhDAxdX5xCKF/rXNTQggfhhCWNEQtkiRJUmOrd4gOIWQAtwKjgF7AeSGEXjt0GwX0SN0uBW6vde6PwMj61iFJklqOtWvXcttttwEwZ84cTj/99DRXJCXTEFeiBwHLY4wrY4xfAPcDZ+zQ5wzgT3Grl4COIYQuADHGucAnDVCHJElqIWqHaKklaogQnQ28V+v4/VRb0j5fKYRwaQihJIRQsmbNmt0qVJIkNQ/FxcWsWLGC3NxcioqK2LBhA2effTZHHnkkY8aMIcYIQGlpKSeccAIDBgygoKCA1atXp7lyaauGCNGhjra4G32+UoxxcowxP8aY37lz5yRDJUlSMzN+/HgOP/xwFi1axIQJEygrK+Pmm2/mtddeY+XKlcybN4/Kykp+8pOf8NBDD1FaWspFF13EuHHj0l26BECbBniM94FDah13A1btRh9JktTKzSirYMLsZbzzTjmffPQZM8oq6AgMGjSIbt26AZCbm0t5eTkdO3ZkyZIlnHzyyQBUV1fTpUuX9BUv1dIQIXoh0COE0B2oAM4Fvr9Dn0eAK0II9wNHA+tijP4+RpKkPciMsgrGTn+VjZXVAFRVb2Hs9FcZc+h62rVrV9MvIyODqqoqYoz07t2b+fPnp6tkaafqvZwjxlgFXAHMBl4HHogxLg0hXB5CuDzVbRawElgO3AH867bxIYT7gPlAzxDC+yGEH9e3JkmS1PxMmL2sJkCHtlls+WIjGyuruX/he3X279mzJ2vWrKkJ0ZWVlSxdurTJ6pW+SkNciSbGOIutQbl226Ra9yPwbzsZe15D1CBJkpq3VWs31tzPyNqPdtm9WHXXvxLatOOwAUd8qX/btm156KGHuPLKK1m3bh1VVVVcddVV9O7duynLluoUtn36tSXJz8+PJSUl6S5DkiQlMGT8s1TUCtLbZHfMYl7x8DRUJH29EEJpjDF/x3a/9luSJDWJooKeZGVmbNeWlZlBUUHPNFUk7b4GWc4hSZL0dQrztn5FxITZy1i1diNdO2ZRVNCzpl1qSQzRkiSpyRTmZRua1Sq4nKMR+ZWmkiRJrZMhuhEZoiVJklonl3MkdO2113LAAQfw05/+FIBx48Zx0EEHsXnzZh544AE2b97MmWeeyQ033EBxcTErVqwgNzeXk08+mQkTJqS5ekmSJDUEr0Qn9OMf/5h77rkHgC1btnD//fdz0EEH8dZbb/Hyyy+zaNEiSktLmTt3LuPHj+fwww9n0aJFBmhJkqRWxCvRu2BGWcV2nyTe0rYDZWVlfPDBB+Tl5bFw4UKefPJJ8vLyANiwYQNvvfUWhx56aJorlyRJUmMwRH+NGWUVjJ3+as3XlFas3Uhl1yFcO+EW9qnewEUXXcQzzzzD2LFjueyyy7YbW15enoaKJUmS1NhczvE1JsxeVhOgt2lz+NE889STLFy4kIKCAgoKCpgyZQobNmwAoKKigg8//JB9992X9evXp6NsSZIkNSKvRH+NVXV8PWnIyKRNtxy+W9CPjIwMTjnlFF5//XWOOeYYADp06MBf/vIXDj/8cIYMGUJOTg6jRo1yXbQkSVIrEWKM6a4hsfz8/FhSUtIkzzVk/LNU7BCkY9zCR3++mkXPP06PHj2apA5JkiQ1vRBCaYwxf8d2l3N8jaKCnmRlZtQcf/HRu6yefCknjRhhgJYkSdpDGaK/RmFeNr8+qw/ZHbMIQPdv9eSBZ0uY+ef/SXdp0i777W9/S05ODjk5Odx8882Ul5dz1FFHcckll9C7d29OOeUUNm7c+huXFStWMHLkSAYMGMDxxx/PG2+8kebqJUlqflzOIbVypaWlXHjhhbz00kvEGDn66KP5y1/+wsCBAykpKSE3N5fvfve7jB49mh/84AeMGDGCSZMm0aNHDxYsWMDYsWN59tln0/0yJElKi50t5/CDhVIrtW1/8zeevp+9D8zlqTfXUpiXzVlnncULL7xA9+7dyc3NBWDAgAGUl5ezYcMG/va3v3HOOefUPM7mzZvT9AokSWq+DNFSK1R7f/MYYf2mKsZOf3W7Pu3atau5n5GRwcaNG9myZQsdO3Zk0aJFTVyxJEkti2uipVao9v7m7Q7pzedvvcRnn3/G+P+7iL/+9a8cf/zxdY7bb7/96N69Ow8++CAAMUZeeeWVJqtbkqSWwhAttUK19zdvd/C36JAzgv/907/z9z/8KxdffDGdOnXa6dipU6dy11130a9fP3r37s3MmTObomRJkloUP1gotUJ17W8OkN0xi3nFw9NQkSRJLZP7REt7kB33NwfIysygqKBnmiqSJKl18YOFUitUmJcNbF0bvWrtRrp2zKKooGdNuyRJqh9DtNRKFeZlG5olSWokLueQJEmSEjJES5IkSQkZoiVJkqSEDNGSJElSQoZoSZIkKSFDtCRJkpSQIVqSJElKyBAtSZIkJWSIliRJkhIyREuSJEkJGaIlSZKkhAzRkiRJUkKGaEmSJCkhQ7QkNWNVVVXpLkGSVAdDtCQ1gvLyco488kguvvhicnJyGDNmDE8//TRDhgyhR48evPzyy3zyyScUFhbSt29fBg8ezOLFiwG4/vrrufTSSznllFM4//zzWbNmDd/5zncYOHAgAwcOZN68eWl+dZKkNukuQJJaq+XLl/Pggw8yefJkBg4cyL333suLL77II488wq9+9SsOOeQQ8vLymDFjBs8++yznn38+ixYtAqC0tJQXX3yRrKwsvv/973P11Vdz3HHH8e6771JQUMDrr7+e3hcnSXs4Q7QkNaAZZRVMmL2Md94pJ7Pjwayo2p8+e+1F7969GTFiBCEE+vTpQ3l5Oe+88w4PP/wwAMOHD+fjjz9m3bp1AIwePZqsrCwAnn76aV577bWa5/j0009Zv349++67b9O/QEkSYIiWpAYzo6yCsdNfZWNlNQDVIYOx018FYK+99qJdu3Y196uqqmjT5st/BYcQANhnn31q2rZs2cL8+fNrQrUkKf1cEy1JDWTC7GU1AXqbjZXVTJi9rM7+Q4cOZerUqQDMmTOHAw44gP322+9L/U455RRuueWWmuNtSz4kSeljiJakBrJq7cZE7ddffz0lJSX07duX4uJi7rnnnjr7TZw4saZfr169mDRpUoPVLEnaPSHGmO4aEsvPz48lJSXpLkOStjNk/LNU1BGYsztmMa94eBoqkiTVVwihNMaYv2O7V6IlqYEUFfQkKzNju7aszAyKCnqmqSJJUmPxg4WS1EAK87KBrWujV63dSNeOWRQV9KxplyS1Hg0SokMII4HfAxnAnTHG8TucD6nzpwKfAxfGGP++K2MlqSUpzMs2NEvSHqDeyzlCCBnArcAooBdwXgih1w7dRgE9UrdLgdsTjJUkSZKalYZYEz0IWB5jXBlj/AK4Hzhjhz5nAH+KW70EdAwhdNnFsZIkSVKz0hAhOht4r9bx+6m2XemzK2MBCCFcGkIoCSGUrFmzpt5FS5IkSburIUJ0qKNtx33zdtZnV8ZubYxxcowxP8aY37lz54QlSpIkSQ2nIT5Y+D5wSK3jbsCqXezTdhfGSpIkSc1KQ1yJXgj0CCF0DyG0Bc4FHtmhzyPA+WGrwcC6GOPqXRwrSZIkNSv1vhIdY6wKIVwBzGbrNnVTYoxLQwiXp85PAmaxdXu75Wzd4u5HXzW2vjVJkiRJjcmv/VaDmjNnDjfddBOPPvpoukuRJEmqN7/2W5IkSWoghuhW5rPPPuO0006jX79+5OTkMG3aNEpLSznhhBMYMGAABQUFrF69GoDly5dz0kkn0a9fP/r378+KFSuIMVJUVEROTg59+vRh2rRpwNYrzMOGDePss8/myCOPZMyYMWz7LcYTTzzBkUceyXHHHcf06dPT9tolSZKaSoN87beajyeeeIKuXbvy2GOPAbBu3TpGjRrFzJkz6dy5M9OmTWPcuHFMmTKFMWPGUFxczJlnnsmmTZvYsmUL06dPZ9GiRbzyyit89NFHDBw4kKFDhwJQVlbG0qVL6dq1K0OGDGHevHnk5+dzySWX8Oyzz/Ktb32L733ve+l8+ZIkSU3CEN0KzCirYMLsZaxau5FOlRuomDWb/a+5htNPP51OnTqxZMkSTj75ZACqq6vp0qUL69evp6KigjPPPBOA9u3bA/Diiy9y3nnnkZGRwUEHHcQJJ5zAwoUL2W+//Rg0aBDdunUDIDc3l/Lycjp06ED37t3p0aMHAD/4wQ+YPHlyGt4FSZKkpmOIbuFmlFUwdvqrbKysBuCTzAP4xnn/zeZ9VzN27FhOPvlkevfuzfz587cb9+mnn9b5eF/1QdN27drV3M/IyKCqqgqAEOr6zhxJkqTWyzXRLdyE2ctqAjRA1fqP2UwbFrbJ4Wc/+xkLFixgzZo1NSG6srKSpUuXst9++9GtWzdmzJgBwObNm/n8888ZOnQo06ZNo7q6mjVr1jB37lwGDRq00+c/8sgjefvtt1mxYgUA9913X+O9WEmSpGbCK9Et3Kq1G7c7rlxTzodz7mZ1CNx46D9x++2306ZNG6688krWrVtHVVUVV111Fb179+bPf/4zl112Gddddx2ZmZk8+OCDnHnmmcyfP59+/foRQuA3v/kNBx98MG+88Uadz9++fXsmT57MaaedxgEHHMBxxx3HkiVLmuKlS5IkpY37RLdwQ8Y/S8UOQRogu2MW84qHp6EiSZKk1sN9olupooKeZGVmbNeWlZlBUUHPNFUkSZLU+rmco4UrzMsGqNmdo2vHLIoKeta0S5IkqeEZoluBwrxsQ7MkSVITcjmHJEmSlJAhWpIkSUrIEC1JkiQlZIiWJEmSEjJES5IkSQkZoiVJkqSEDNGSJElSQoZoSZIkKSFDtCRJkpSQIVqSJElKyBAtSZIkJWSIliRJkhIyREuSJEkJGaIlSZKkhAzRkiRJUkKGaEmSJCkhQ7QkSZKUkCFakiRJSsgQLUmSJCVkiJYkSZISMkRLkiRJCRmiJUmSpIQM0ZIkSVJChmhJkiQpIUO0JEmSlJAhWpIkSUrIEC1JkiQlZIiWJEmSEjJES5IkSQkZoiVJkqSEDNGSJElSQoZoSZIkKSFDtCRJkpSQIVqSJElKyBAtSZIkJVSvEB1C2D+E8FQI4a3Uz0476TcyhLAshLA8hFBcq/2cEMLSEMKWEEJ+fWqRJEmSmkp9r0QXA8/EGHsAz6SOtxNCyABuBUYBvYDzQgi9UqeXAGcBc+tZhyRJktRk6huizwDuSd2/Byiso88gYHmMcWWM8Qvg/tQ4YoyvxxiX1bMGSZIkqUnVN0QfFGNcDZD6eWAdfbKB92odv59qSySEcGkIoSSEULJmzZrdKlaSJElqCG2+rkMI4Wng4DpOjdvF5wh1tMVdHPv/B8Q4GZgMkJ+fn3i8JEmS1FC+NkTHGE/a2bkQwgchhC4xxtUhhC7Ah3V0ex84pNZxN2BV4kolSZKkZqK+yzkeAS5I3b8AmFlHn4VAjxBC9xBCW+Dc1DhJkiSpRapviB4PnBxCeAs4OXVMCKFrCGEWQIyxCrgCmA28DjwQY1ya6ndmCOF94BjgsRDC7HrWI0mSJDW6EGPLW16cn58fS0pK0l2GJEmSWrkQQmmM8UvfZ+I3FkqSJEkJGaIlSZKkhAzRkiRJUkKGaEmSJCkhQ7QkSZKUkCFakiRJSsgQLUmSJCVkiJYkSZISMkRLkiRJCRmiJUmSpIQM0ZIkSVJChmhJkiQpIUO0JEmSlJAhWpIkSUrIEC1JkiQlZIiWJEmSEjJES5IkSQkZoiVJkqSEDNGSJElSQoZoSZIkKSFDtCRJkpSQIVqSpFZs7dq13HbbbQDMmTOH008/vc5+F198Ma+99lpTlia1aIZoSZJasdoh+qvceeed9OrVqwkqkloHQ7QkSa1YcXExK1asIDc3l6KiIjZs2MDZZ5/NkUceyZgxY4gxAjBs2DBKSkqorq7mwgsvJCcnhz59+vC73/0uza9Aap7apLsASZLUeMaPH8+SJUtYtGgRc+bM4YwzzmDp0qV07dqVIUOGMG/ePI477ria/osWLaKiooIlS5YAW69kS/oyQ7QkSa3MjLIKJsxexqq1G9k/ruPTTVU15wYNGkS3bt0AyM3Npby8fLsQ/c1vfpOVK1fyk5/8hNNOO41TTjmlyeuXWgKXc0iS1IrMKKtg7PRXqVi7kQh88OkmPvh0EzPKKgBo165dTd+MjAyqqqq2G9+pUydeeeUVhg0bxq233srFF1/clOVLLYYhWpKkVmTC7GVsrKyuOQ5ts6je/DkTZi/bpfEfffQRW7Zs4Tvf+Q6//OUv+fvf/95YpUotmss5JElN4thjj+Vvf/tbusto9Vat3bjdcUbWfrTL7sXC//4RRYcdyEEHHfSV4ysqKvjRj37Eli1bAPj1r3/daLVKLVnY9qncliQ/Pz+WlJSkuwxJkpqdIeOfpWKHIA2Q3TGLecXD01CR1LKFEEpjjPk7trucQ5LUJDp06ADA6tWrGTp0KLm5ueTk5PDCCy+kubLWpaigJ1mZGdu1ZWVmUFTQM00VSa2TyzkkSU3q3nvvpaCggHHjxlFdXc3nn3+e7pJalcK8bICa3Tm6dsyiqKBnTbukhmGIliQ1mtpbrW2srGZGWQUDBw7koosuorKyksLCQnJzc9NdZqtTmJdtaJYamcs5JEmNYset1mKEsdNf5ZN9D2fu3LlkZ2fzwx/+kD/96U/pLlWSEjNES5IaxY5brQFsrKzml/fP5cADD+SSSy7hxz/+sVuoSWqRXM4hSWoUO261ts27SxaSm3sjmZmZdOjQwSvRklokQ7QkqVF07Zi13VZrh/77QwAccfzpzHvst+kqS5IahMs5JEmNwq3WJLVmXomWJDUKt1qT1JoZoiVJjcat1iS1Vi7nkCRJkhIyREuSJEkJGaIlSZKkhAzRkiRJUkKGaEmSJCkhQ7QkSZKUUL1CdAhh/xDCUyGEt1I/O+2k38gQwrIQwvIQQnGt9gkhhDdCCItDCH8NIXSsTz2SJElSU6jvlehi4JkYYw/gmdTxdkIIGcCtwCigF3BeCKFX6vRTQE6MsS/wJjC2nvVIkiRJja6+IfoM4J7U/XuAwjr6DAKWxxhXxhi/AO5PjSPG+GSMsSrV7yWgWz3rkSRJkhpdfUP0QTHG1QCpnwfW0ScbeK/W8fupth1dBDy+sycKIVwaQigJIZSsWbOmHiVLkiRJ9fO1X/sdQngaOLiOU+N28TlCHW1xh+cYB1QBU3f2IDHGycBkgPz8/LizfpIkSVJj+9oQHWM8aWfnQggfhBC6xBhXhxC6AB/W0e194JBax92AVbUe4wLgdGBEjNFwLEmSpGavvss5HgEuSN2/AJhZR5+FQI8QQvcQQlvg3NQ4QggjgWuA0THGz+tZiyRJktQk6huixwMnhxDeAk5OHRNC6BpCmAWQ+uDgFcBs4HXggRjj0tT4W4B9gadCCItCCJPqWY8kSZLU6L52OcdXiTF+DIyoo30VcGqt41nArDr6fas+zy9Jzc0f//hHSkpKuOWWW/jtb3/LnXfeSZs2bejcuTNTpkzhn//5n9NdoiSpAfiNhZJUD9XV1Ts9l5eXR0lJCYsXL+bss8/m5z//eRNWJklqTIZoSXus3/zmN0ycOBGAq6++muHDhwPwzDPP8IMf/ID77ruPPn36kJOTwzXXXFMzrkOHDlx33XUcffTRzJ8/n7vvvpsjjjiCE044gXnz5tX0O/HEE9l7770BGDx4MO+//z4A3/ve95g16///cu7CCy/k4Ycfprq6mqKiIgYOHEjfvn35n//5n+1q7dOnD/369aO4+EvfayVJamKGaEl7rKFDh/LCCy8AUFJSwoYNG6isrOTFF1+kR48eXHPNNTz77LMsWrSIhQsXMmPGDAA+++wzcnJyWLBgAYcffjj/+Z//ybx583jqqad47bXX6nyuu+66i1GjRgFw7rnnMm3aNAC++OILnnnmGU499VTuuusuvvGNb7Bw4UIWLlzIHXfcwdtvv83jjz/OjBkzWLBgAa+88opXtCWpGajXmmhJamlmlFUwYfYyVq3dyMH7ZvL2/JdZv3497dq1o3///pSUlPDCCy/w7W9/m2HDhtG5c2cAxowZw9y5cyksLCQjI4PvfOc7ACxYsGC7ft/73vd48803t3vOv/zlL5SUlPD8888DMGrUKK688ko2b97ME088wdChQ8nKyuLJJ59k8eLFPPTQQwCsW7eOt956i6effpof/ehHNVe1999//yZ5ryRJO2eIlrTHmFFWwdjpr7Kxcus65tXrK1nfphNX//J3HHvssfTt25fnnnuOFStWcOihh1JaWlrn47Rv356MjIya4xDq+k6prZ5++mluvPFGnn/+edq1a1czftiwYcyePZtp06Zx3nnnARBj5A9/+AMFBQXbPcYTTzzxlc8hSWp6LueQtMeYMHtZTYDeJrNbL/48+VaGDh3K8ccfz6RJk8jNzWXw4ME8//zzfPTRR1RXV3PfffdxwgknfOkxjz76aObMmcPHH39MZWUlDz74YM25srIyLrvsMh555BEOPPDA7cade+653H333bzwwgs1obmgoIDbb7+dyspKAN58800+++wzTjnlFKZMmcLnn2/dTv+TTz5p0PdFkpScIVrSHmPV2o1famvXrTdfrP+YY445hoMOOoj27dtz/PHH06VLF379619z4okn0q9fP/bdd1969OhRM27YsGGUlJTQpUsXrr/+eo455hhOOukk+vfvX9OnqKiIDRs2cM4555Cbm8vo0aNrzp1yyinMnTuXk046ibZt2wJw8cUX06tXL/r3709OTg6XXXYZVVVVjBw5ktGjR5Ofn09ubi433XRTI75LkqRdEVriN23n5+fHkpKSdJchqYUZMv5ZKuoI0tkds5hXPPwrx1544YWcfvrpnH322cDWEH3TTTeRn5+fuI7q6urtloNIkpqvEEJpjPFLf9l7JVpSq1dYWMiAAQN4745/YfOrswF497dn84+5f+J/7/4JH039GR988AEA77zzDiNGjKBv376MGDGCd999l7/97W888sgjFBUVkZuby4oVKwB48MEHGTRoEEcccUTNLh8726Zuzpw5nHjiiXz/+9+nT58+aXgXJEkNyRAtqdWbMmUKpaWlLFuyiMxlT3Jg5hfEyk107dGH+594gTNGncQdd9wBwBVXXMH555/P4sWLGTNmDFdeeSXHHnsso0ePZsKECSxatIjDDz8cgKqqKl5++WVuvvlmbrjhBoCdblMH8PLLL3PjjTfudBs8SVLL4e4cklql2lvZVZU8QJt3F7JfVibr1qzmgdO6cMJv2vLaH8cRQmDzmwN46qmnAJg/fz7Tp08H4Ic//OFX7sl81llnATBgwADKy8sBdrpNXdu2bRk0aBDdu3dvxFctSWoqhmhJrU7trew2vbuYtW+U8M8/+BU3fG8gN189hk2bNpGZmVmzbVxGRgZVVVV1PtZXbS23bcu62uN3tk3dnDlz2GeffRri5UmSmgGXc0hqdWpvZbdl8+fs1X4fNpPJDX9+ipdeeukrxx577LHcf//9AEydOpXjjjsOgH333Zf169d/7XPvbJs6SVLrYoiW1OrU3souq/sA4pYtrJpyBW/OupPBgwd/5diJEydy991307dvX/785z/z+9//Hti6r/OECRPIy8ur+WBhXXa2TZ0kqXVxiztJrU59trKTJKk2t7iTtMcoKuhJVub2+zBnZWZQVNAzTRVJklobP1goqdUpzMsGqNmdo2vHLIoKeta0S5JUX4ZoSa1SYV62oVmS1GhcziFJkiQlZIiWJEmSEjJES5IkSQkZoiVJkqSEDNGSJElSQoZoSZIkKSFDtCRJkpSQIVqSJElKyBAtSZIkJWSIliRJkhIyREuSJEkJGaIlSZKkhAzRkiRJUkKGaEmSJCkhQ7QkSZKUkCFakiRJSsgQLUmSJCVkiJYkSZISMkRLkiRJCRmiJUmSpIQM0ZIkSVJChmhJkiQpIUO0JEmSlJAhWpIkSTvVoUOHdJfQLBmiJUmSpIQM0ZIkSa1cYWEhAwYMoHfv3kyePBnYeoV53Lhx9OvXj8GDB/PBBx8A8Pbbb3PMMccwcOBArr322nSW3awZoiVJklq5KVOmUFpaSklJCRMnTuTjjz/ms88+Y/DgwbzyyisMHTqUO+64A4Cf/vSn/Mu//AsLFy7k4IMPTnPlzZchWpIkqZWbOHFizRXn9957j7feeou2bdty+umnAzBgwADKy8sBmDdvHueddx4AP/zhD9NVcrPXpj6DQwj7A9OAw4By4Lsxxn/U0W8k8HsgA7gzxjg+1f5L4AxgC/AhcGGMcVV9apIkSdrTzSirYMLsZaxau5F9PllG9cJZlM6fz957782wYcPYtGkTmZmZhBAAyMjIoKqqqmb8tnbtXH2vRBcDz8QYewDPpI63E0LIAG4FRgG9gPNCCL1SpyfEGPvGGHOBR4Hr6lmPJEnSHm1GWQVjp79KxdqNRODDj//Be58Fnlz2D9544w1eeumlrxw/ZMgQ7r//fgCmTp3aBBW3TPUN0WcA96Tu3wMU1tFnELA8xrgyxvgFcH9qHDHGT2v12weI9axHkiRpjzZh9jI2VlbXHGd1H0B1VTVjTj2ea6+9lsGDB3/l+N///vfceuutDBw4kHXr1jV2uS1WiHH3c2sIYW2MsWOt43/EGDvt0OdsYGSM8eLU8Q+Bo2OMV6SObwTOB9YBJ8YY1+zkuS4FLgU49NBDB7zzzju7XbckSVJr1b34sTqvSgbg7fGnNXU5LV4IoTTGmL9j+9deiQ4hPB1CWFLH7Yxdfe462mrmNsY4LsZ4CDAVuGJnDxJjnBxjzI8x5nfu3HkXn1qSJGnP0rVjVqJ27Z6vDdExxpNijDl13GYCH4QQugCkfn5Yx0O8DxxS67gbUNeHB+8FvpP8JUiSJGmbooKeZGVmbNeWlZlBUUHPNFXUOtV3TfQjwAWp+xcAM+vosxDoEULoHkJoC5ybGkcIoUetfqOBN+pZjyRJ0h6tMC+bX5/Vh+yOWQQgu2MWvz6rD4V52ekurVWp1xZ3wHjggRDCj4F3gXMAQghd2bqV3akxxqoQwhXAbLZucTclxrh02/gQQk+2bnH3DnB5PeuRJEna4xXmZRuaG1m9PliYLvn5+bGkpCTdZUiSJKmV2+0PFkqSJEnaniFakiRJSsgQLUmSJCVkiJYkSZISMkRLkiRJCRmiJUmSpIQM0ZIkSVJChmhJkiQpIUO0JEmSlJAhWpIkSUrIEC1JkiQlFGKM6a4hsRDCGuCdNDz1AcBHaXhefZlz0Xw4F82Hc9F8OBfNi/PRfLTEufjnGGPnHRtbZIhOlxBCSYwxP911yLloTpyL5sO5aD6ci+bF+Wg+WtNcuJxDkiRJSsgQLUmSJCVkiE5mcroLUA3novlwLpoP56L5cC6aF+ej+Wg1c+GaaEmSJCkhr0RLkiRJCRmiJUmSpIQM0TsIIewfQngqhPBW6mennfQbGUJYFkJYHkIortX+yxDC4hDCohDCkyGErk1XfevSAHMxIYTwRmo+/hpC6NhkxbcyDTAX54QQloYQtoQQWsXWRk1tZ+9trfMhhDAxdX5xCKH/ro5VMvWciykhhA9DCEuaturWaXfnIoRwSAjhuRDC66m/m37a9NW3LvWYi/YhhJdDCK+k5uKGpq9+N8UYvdW6Ab8BilP3i4H/U0efDGAF8E2gLfAK0Ct1br9a/a4EJqX7NbXUWwPMxSlAm9T9/1PXeG9NNhdHAT2BOUB+ul9PS7t91Xtbq8+pwONAAAYDC3Z1rLemmYvUuaFAf2BJul9LS7/V889FF6B/6v6+wJv+uUjbXASgQ+p+JrAAGJzu17QrN69Ef9kZwD2p+/cAhXX0GQQsjzGujDF+AdyfGkeM8dNa/fYB/OTm7qvvXDwZY6xK9XsJ6Na45bZq9Z2L12OMy5qi0FZqp+9tLWcAf4pbvQR0DCF02cWx2nX1mQtijHOBT5q04tZrt+cixrg6xvh3gBjjeuB1ILspi29l6jMXMca4IdUnM3VrEdnJEP1lB8UYVwOkfh5YR59s4L1ax+9T6w9fCOHGEMJ7wBjgukastbWr91zUchFb/wes3dOQc6HkduW93Vkf56Vh1Wcu1LAaZC5CCIcBeWy9AqrdU6+5CCFkhBAWAR8CT8UYW8RctEl3AekQQngaOLiOU+N29SHqaKv5X1OMcRwwLoQwFrgC+M/ERe4hGnsuUs8xDqgCpiarbs/SFHOh3bYr7+3O+jgvDas+c6GGVe+5CCF0AB4GrtrhN8lKpl5zEWOsBnJTn136awghJ8bY7D83sEeG6BjjSTs7F0L4YNuvelK/fvuwjm7vA4fUOu4GrKqj373AYxiid6qx5yKEcAFwOjAiphZcqW5N+OdCye3Ke7uzPm13Yax2XX3mQg2rXnMRQshka4CeGmOc3oh17gka5M9FjHFtCGEOMBJo9iHa5Rxf9ghwQer+BcDMOvosBHqEELqHENoC56bGEULoUavfaOCNRqy1tavvXIwErgFGxxg/b4J6W7N6zYXqbVfe20eA81OfgB8MrEstvXFeGlZ95kINa7fnIoQQgLuA12OMv23aslul+sxF59QVaEIIWcBJtJTslO5PNja3G/BPwDPAW6mf+6fauwKzavU7la2f5l0BjKvV/jBb//e0GPi/QHa6X1NLvTXAXCxn6/qrRambO6Wkby7OZOtViM3AB8DsdL+mlnar670FLgcuT90PwK2p869SaxeUnc2Lt7TMxX3AaqAy9Wfix+l+PS35trtzARzH1qUEi2v9G3Fqul9PS77VYy76AmWpuVgCXJfu17KrN7/2W5IkSUrI5RySJElSQoZoSZIkKSFDtCRJkpSQIVqSJElKyBAtSZIkJWSIliRJkhIyREuSJEkJ/T/D04KFggg/eQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a scatter plot of the projection\n",
    "plt.figure(figsize=(12,7))\n",
    "import matplotlib.pyplot as plt  \n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255c110b-c0be-4e4b-8c46-0121b0890629",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91e75117-f7af-4f06-8755-03525dd9cf35",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m487.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m487.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from spacy) (21.3)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (493 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.0/493.0 kB\u001b[0m \u001b[31m711.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.10-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/nabil/.local/lib/python3.10/site-packages (from spacy) (1.10.4)\n",
      "Collecting typer<0.10.0,>=0.3.0\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m59.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/nabil/.local/lib/python3.10/site-packages (from spacy) (2.28.1)\n",
      "Collecting thinc<8.3.0,>=8.1.8\n",
      "  Downloading thinc-8.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (920 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m920.6/920.6 kB\u001b[0m \u001b[31m811.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.9/156.9 kB\u001b[0m \u001b[31m706.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0\n",
      "  Downloading weasel-0.3.3-py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.8/49.8 kB\u001b[0m \u001b[31m174.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy) (59.6.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/lib/python3/dist-packages (from spacy) (1.21.5)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (46 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m280.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m591.6 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/nabil/.local/lib/python3.10/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/nabil/.local/lib/python3.10/site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/nabil/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/nabil/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.5)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.3-py3-none-any.whl (34 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m518.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/lib/python3/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.3)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m279.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2->spacy) (2.0.1)\n",
      "Installing collected packages: cymem, wasabi, typer, spacy-loggers, spacy-legacy, murmurhash, langcodes, cloudpathlib, catalogue, blis, srsly, preshed, confection, weasel, thinc, spacy\n",
      "Successfully installed blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.3 cymem-2.0.8 langcodes-3.3.0 murmurhash-1.0.10 preshed-3.0.9 spacy-3.7.2 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.1 typer-0.9.0 wasabi-1.1.2 weasel-0.3.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f93b2e-fc50-46c1-896e-50025d5321c5",
   "metadata": {},
   "source": [
    "After installation, you need to download language models using the spacy download command. For example, to download the English language model, you would run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11f8c325-253a-4998-9e12-8b924f0c450c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-lg==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.0/en_core_web_lg-3.7.0-py3-none-any.whl (587.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0mB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:34\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /home/nabil/.local/lib/python3.10/site-packages (from en-core-web-lg==3.7.0) (3.7.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/nabil/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/nabil/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/lib/python3/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (1.21.5)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/nabil/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/nabil/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/nabil/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/nabil/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (6.4.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (59.6.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/nabil/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (1.10.4)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (3.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (21.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/nabil/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/nabil/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (0.3.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/nabil/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/nabil/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (4.64.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/nabil/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /home/nabil/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (8.2.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/nabil/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/nabil/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (0.9.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/nabil/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/nabil/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (4.7.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (1.26.5)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/nabil/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (2.1.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/nabil/.local/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (0.1.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/nabil/.local/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/lib/python3/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (8.0.3)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/nabil/.local/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (2.0.1)\n",
      "Installing collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.7.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "048b0074-e551-4d1c-a6d3-fa934ffc345a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy\n",
      "is\n",
      "a\n",
      "powerful\n",
      "tool\n",
      "for\n",
      "natural\n",
      "language\n",
      "processing\n",
      "fhfdh\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER, and word vectors\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Process a text\n",
    "text = \"Spacy is a powerful tool for natural language processing fhfdh .\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print the tokens\n",
    "for token in doc:\n",
    "    print(token.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6a069d9-d8c6-4afb-99ed-fc1ea035c0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the token Spacy has vector: True ------ OOV ?:  False\n",
      "the token is has vector: True ------ OOV ?:  False\n",
      "the token a has vector: True ------ OOV ?:  False\n",
      "the token powerful has vector: True ------ OOV ?:  False\n",
      "the token tool has vector: True ------ OOV ?:  False\n",
      "the token for has vector: True ------ OOV ?:  False\n",
      "the token natural has vector: True ------ OOV ?:  False\n",
      "the token language has vector: True ------ OOV ?:  False\n",
      "the token processing has vector: True ------ OOV ?:  False\n",
      "the token fhfdh has vector: False ------ OOV ?:  True\n",
      "the token . has vector: True ------ OOV ?:  False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f'the token {token.text} has vector:',token.has_vector, \"------ OOV ?: \",token.is_oov)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82463649-7c95-44f5-a44d-cf5463fbb6d0",
   "metadata": {},
   "source": [
    "In SpaCy, the is_oov method is used to check if a token is out-of-vocabulary (OOV). This method is available on a Token object within a SpaCy Doc. \"Out-of-vocabulary\" typically refers to a word that is not present in the vocabulary used by the specific language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3fdbf92a-4518-4d5f-a726-e533e9849a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0].vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9186d639-2388-4153-937e-575ab1312c8b",
   "metadata": {},
   "source": [
    "## Token Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b4575577-7c08-4e94-9c65-ed675b44260e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'cat' and 'dog': 0.8220816850662231\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Example text\n",
    "text = \"cat dog\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Get the tokens for \"cat\" and \"dog\"\n",
    "token_cat = doc[0]  # Get the token for \"cat\"\n",
    "token_dog = doc[1]  # Get the token for \"dog\"\n",
    "\n",
    "# Calculate similarity between \"cat\" and \"dog\"\n",
    "similarity = token_cat.similarity(token_dog)\n",
    "\n",
    "print(f\"Similarity between 'cat' and 'dog': {similarity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a14ad8f-51c7-4e65-abaa-0a9afc61d113",
   "metadata": {},
   "source": [
    "SpaCy calculates token similarity based on word vectors. Each token in a SpaCy Doc has an associated word vector that represents the word's meaning in a high-dimensional space. This vector is based on the context in which the word appears in the training data used to create the language model.\n",
    "\n",
    "The similarity between two tokens is computed using the cosine similarity between their word vectors. Cosine similarity measures the cosine of the angle between two non-zero vectors. The formula for cosine similarity between two vectors A and B is:\n",
    "\n",
    "$$\\text{Cosine Similarity} = \\frac{{A \\cdot B}}{{\\|A\\| \\times \\|B\\|}}\n",
    " $$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$A \\cdot B $$ represents the dot product of vectors A and B.\n",
    "$$\\|A\\|∥A∥ $$ and $$  \\|B\\|∥B∥ $$ represent the magnitudes (or norms) of vectors A and B, respectively.\n",
    "In the context of token similarity in SpaCy:\n",
    "\n",
    "The dot product of the word vectors for two tokens is calculated.\n",
    "The magnitudes of the individual word vectors are computed.\n",
    "The dot product is divided by the product of the magnitudes to get the cosine similarity value.\n",
    "The resulting value of the similarity falls between -1 and 1, with 1 indicating highly similar word vectors, 0 indicating no similarity, and -1 indicating vectors that are dissimilar but in a mirrored sense.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
